{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Assumptions in Machine Learning\n",
    "\n",
    "Every machine learning model has assumptions, indeed, that is the only way we can reasonably learn from data and make assumptions about how what we learn generalizes to the real world. \n",
    "\n",
    "The problem is the assumptions/difficulty tradeoff. Simpler models tend to have more naive assumptions (ie, assumptions that are not likely to be true or not likely to generalize), and more complex models (such as neural networks, convolutional networks, and recurrent neural networks) have less assumptions (and therefore, save for overfitting, are more likely to generalize better to the real world), difficult (and expensive) to build, train, and deploy. \n",
    "\n",
    "To understand this specifically, lets go through some models and state the assumptions they make. \n",
    "\n",
    "\n",
    "### Linear Regression & Logistic Regression\n",
    "\n",
    "Linear regression assumes that the data can be generated via a linear function, and logistic regression assumes that the data can be linearly separated. \n",
    "\n",
    "![lin](http://sebastianraschka.com/images/blog/2014/kernel_pca/linear_vs_nonlinear.png)\n",
    "\n",
    "Although we may intuitively think that assuming linearity is a very naive assumption, these are two of the most common algorithms used in the industry to make reasonable predictions for the following reasons: \n",
    "\n",
    "1. They are relatively easy to train, and don't require expensive & difficult research to optimize\n",
    "2. They have well known theoretical gaurantees and have been shown to work well in practice for decades\n",
    "3. Domain expertise can be applied to make sense of what has been learned\n",
    "4. The assumptions often turn out to be mostly true, and the tradeoff we have to make to get the edge cases classified correctly aren't worth the extra work.\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "Bayes Rule states that for a latent variable and an evidence variable, the posterior distribution of the latent variable is equivalent to the prior probability of the latent variable times the likelihood of the evidence given the latent variable, divided by a constant (the probability of observing the evidence). \n",
    "\n",
    "Specifically:\n",
    "\n",
    "$$ p(y_i | x) = \\frac{p(y_i) p(x | y_i)}{p(x)} = \\frac{p(x, y_i)}{\\sum_i p(x, y_i)} $$\n",
    "\n",
    "Using the rules of probability, we can write the joint distribution as a conditional:\n",
    "\n",
    "$$ p(x, y_i) = p(x_1 | x_2, ... x_n, y_i) * p(x_2 | x_3, ... x_n, y_i) $$\n",
    "\n",
    "Here is where the assumption comes into play: since we can't really compute these conditional probabilities, we just define: \n",
    "\n",
    "$$ p(x_1 | x_2, ... x_n, y_i) = p(x_1 | y_i) $$\n",
    "\n",
    "This assumption means that each feature in our training set is entirely dependent upon the label that it is given, and is completely independent of every other feature, given the class label.\n",
    "\n",
    "We cal also set $ p(x_1 | y_i) $ to take on whatever probability distribution we want to (such as the Gaussian); doing this is also another assumption in our model. \n",
    "\n",
    "### Neural Networks\n",
    "- They also assume that the data are conditionally indpendent of each other. \n",
    "\n",
    "\n",
    "We have used Neural Networks and Convolutional Neural Netoworks to remove the assumptions of linearity in our data (and for convolutional neural networks, the assumption of spatial variance being important). Now we will use Recurrent Neural Networks to remove the assumption of independence (to an extent). \n",
    "\n",
    "Aside: A much simpler model (that doesn't work as well, but still has produced good results) that removes this independence assumption are Hidden Markov Models.\n",
    "\n",
    "\n",
    "\n",
    "### An Introduction to Recurrent Neural Networks in Tensorflow\n",
    "\n",
    "Recurrent Neural Networks are models that are able to model relationships between data where each input is not  independent of the previous inputs. \n",
    "\n",
    "For example, consider the field of Natural Language Processing (NLP). NLP deals with all tasks involved with processing texts and obtaining information (learning) from text. It is extremely common, especially today, to solve NLP tasks with machine learning (and especially deep learning). \n",
    "\n",
    "Recurrent Neural networks are a popular method for solving NLP problems, including predicting the sentiment of a movie review, predicting the next word (or character) in a sequence of words or characters, and even translating text to another language. This is because they can model dependencies, and often in text a current word is dependent on the previous words (you can probably think of several examples). \n",
    "\n",
    "Just for a break from all this technical talk, here is some Shakespeare: \n",
    "\n",
    "```\n",
    "PANDARUS:\n",
    "Alas, I think he shall be come approached and the day\n",
    "When little srain would be attain'd into being never fed,\n",
    "And who is but a chain and subjects of his death,\n",
    "I should not sleep.\n",
    "\n",
    "Second Senator:\n",
    "They are away this miseries, produced upon my soul,\n",
    "Breaking and strongly should be buried, when I perish\n",
    "The earth and thoughts of many states.\n",
    "\n",
    "DUKE VINCENTIO:\n",
    "Well, your wit is in the care of side and that.\n",
    "\n",
    "Second Lord:\n",
    "They would be ruled after this chamber, and\n",
    "my fair nues begun out of the fact, to be conveyed,\n",
    "Whose noble souls I'll have the heart of the wars.\n",
    "\n",
    "Clown:\n",
    "Come, sir, I will make did behold your worship.\n",
    "\n",
    "VIOLA:\n",
    "I'll drink it.\n",
    "```\n",
    "\n",
    "\n",
    "### The Power of Recurrent Neural Networks\n",
    "\n",
    "I lied, the above was not really Shakespeare. It was actually a poem written by a recurrent neural network that trained on a dataset of Shakespeare's writings. This should give you an idea of the power of RNNs: just by looking at characters (not even words!) it learned how to generate strikingly similar text to Shakespeare. Granted, it does not make much sense but it is likely that at least engineers will not be able to tell the difference.\n",
    "\n",
    "This was from an awesome article here http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "Recurrent Neural Networks are important because they model sequential data. Each neuron, in addition to its data input, receives a previous ** state ** which is just a vector of numbers describing some previous computation in the RNN. This state input is what allows RNNs to learn dependencies in sequential data. \n",
    "\n",
    "It's a lot similar to you reading this sentence - when you understand the meaning of a particular word, you don't start figuring it out from scratch, but take the previous words and sentences into account. \n",
    "\n",
    "Here is what an RNN looks like: \n",
    "\n",
    "![rnn](https://cdn-images-1.medium.com/max/1600/1*V2W4TCmTj2h1CE7I-DngPw.png)\n",
    "\n",
    "\n",
    "And this is what an RNN looks like, predicting words character by character: \n",
    "\n",
    "![rnn-2](https://cdn-images-1.medium.com/max/1600/1*IMalbwl6uj3nlqxixZYFvA.jpeg)\n",
    "\n",
    "To understand RNNs deeply, we'll need to get into some data and code. \n",
    "\n",
    "### The data\n",
    "\n",
    "To clearly demonstrate the fundamental idea of RNNs learning dependencies in data, we will be using numerical data with clearly observable and quantifiable dependencies, and then we'll be able to check if the RNN can learn these dependencies. \n",
    "\n",
    "All of the code is written in a general enough format that words (represented as one-hot indices into a vocabulary or learned word vectors) can be fed into this model, however.\n",
    "\n",
    "\n",
    "**Input Sequence (X)** The input sequence is just a bunch of ones and zeros generated randomly. At each index, or more appropriately for RNNs, at each time *t* $X_t$ is 1 with probability 0.5 else 0. \n",
    "\n",
    "**Example:** $ X = [1, 0, 1, 1, 0, 0, ...] $\n",
    "\n",
    "**output Sequence (Y)** The output sequence will be generated from the input sequence, and our recurrent neural network will have to learn the rules by which the output sequence is generated. \n",
    "\n",
    "At each step in time $t$, $Y_t$ starts off as 1 with an initial probability of $0.5$. However if $X_{t-3} = 1$ then it $Y_t$ is almost surely 1 (ie, we raise the probability $p(y_t = 1)$ by 0.5 \n",
    "\n",
    "But, if $X_{t-8}$ is also 1 then we will decrease $P(y_t = 1)$ by 0.25. \n",
    "\n",
    "Hopefully, it is clear that if $X_{t-3} = 1$ and $X_{t-8} = 1$ then $y_t = 1 $ with probability 0.75. \n",
    "\n",
    "Note that there are no rules (ie, changes in $p(y_i = 1)$ if X is 0. \n",
    "\n",
    "This data is pretty simple. In fact, it is so simple that we can calculate exactly what our RNN should learn (given enough iterations and data) in order to determine if it has learned no dependencies, the dependency from 3 time steps ago, and the dependency from 8 time steps ago. \n",
    "\n",
    "#### Network Learns no dependencies\n",
    "\n",
    "To calculate the probability with which our network will predict $y_t = 1$, we can look at our generated data. \n",
    "\n",
    "First, in our generated data, $y_t = 1$ with initial probability 0.5. But we added 0.5 if $x_{t-3} = 1$, which itself has a probability $0.5$. So now, $p(y_t = 1) = 0.5 + 0.5^2 = 0.75$. However, we also need to remove .25 if $x_{t-8} = 1$, which has a probability of $0.5$. \n",
    "\n",
    "So in our generated data, $p(y_t = 1) = 0.5 + 0.5^2 - 0.25*0.5 = 0.625 $\n",
    "\n",
    "Since our network does not know any conditional dependence on our input and output data, it will just learn the distribution assuming that $y_t$ is independent of all $X$. So our network will assign every $y_t$ a probability of 0.625.\n",
    "\n",
    "The cross entropy loss that corresponds to this is $ -(.625 * log(.625) + (1 - .625) * log(1 - .625)) = 0.66$. \n",
    "\n",
    "#### Network Learns one dependency\n",
    "\n",
    "If our network realizes that whenever $x_{t-3} = 1$, the probability $p(y_t) = 1$ goes up by 0.5, then it will assign a probability of $0.875$ when $x_{t-3} = 1$, because $p(y_t = 1) = 0.625 + 0.5*.5 = 0.875$ and a probability of $0.625$ when $x_{t-3} = 1$. \n",
    "\n",
    "The corresponding cross entropy loss is $ -0.5 * (0.875 * log(0.875) + 0.125 * log(0.125)) + -0.5 * (0.625 * log(0.625) + 0.375 * log(0.375)))  = 0.52$\n",
    "\n",
    "#### Neural Network learns both dependencies\n",
    "\n",
    "Exercise: work out the probabilities with which our network will assign $p(y_t = 1)$, and calculate the cross-entropy. \n",
    "\n",
    "To check your answer, the cross entropy you should get is 0.45. \n",
    "\n",
    "\n",
    "So now we know that if we train our network correctly: \n",
    "\n",
    "- With one hidden state, we should learn no dependencies, so our final loss should be 0.66. \n",
    "\n",
    "- With three or more hidden states, we should learn the first dependency, so our final loss should be 0.52. \n",
    "\n",
    "- With eight or more hidden states, we should learn both dependencies, so our final loss should be 0.46. \n",
    "\n",
    "Let's start building our RNN to see if our hypothesis is supported. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from gen_data import gen_data # gets the data for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RNNconfig = {\n",
    "    'num_steps' : 5, # higher n = capture longer term dependencies, but more expensive (and potential vanishing gradient issues)\n",
    "    'batch_size' : 200,\n",
    "    'state_size' :4,\n",
    "    'learning_rate' : 0.1\n",
    "}\n",
    "\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), RNNconfig['batch_size'], num_steps) # python trivia: why do we use yield instead of return here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.int32, [RNNconfig['batch_size'], RNNconfig['num_steps']], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [RNNconfig['batch_size'], RNNconfig['num_steps']], name='labels_placeholder')\n",
    "init_state = tf.zeros([RNNconfig['batch_size'], RNNconfig['state_size']])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes) # note: num_classes is not an RNN variable...\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom __call__ method\\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from __call__ method\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py\n",
    "\"\"\"\n",
    "# with tf.variable_scope('rnn_cell'):\n",
    "#     W = tf.get_variable('W', [num_classes + RNNconfig['state_size'], RNNconfig['state_size']])\n",
    "#     b = tf.get_variable('b', [RNNconfig['state_size']], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# def rnn_cell(rnn_input, state):\n",
    "#     with tf.variable_scope('rnn_cell', reuse=True):\n",
    "#         W = tf.get_variable('W', [num_classes + RNNconfig['state_size'], RNNconfig['state_size']])\n",
    "#         b = tf.get_variable('b', [RNNconfig['state_size']], initializer=tf.constant_initializer(0.0))\n",
    "#     return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py\n",
    "\"\"\"\n",
    "# state = init_state\n",
    "# rnn_outputs = []\n",
    "# for rnn_input in rnn_inputs:\n",
    "#     state = rnn_cell(rnn_input, state)\n",
    "#     rnn_outputs.append(state)\n",
    "# final_state = rnn_outputs[-1]\n",
    "cell = tf.contrib.rnn.BasicLSTMC(RNNconfig['state_size'])\n",
    "\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state = init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [RNNconfig['state_size'], num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=RNNconfig['num_steps'], axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(0.1).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_network(num_epochs, verbose=True):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, RNNconfig['num_steps'])):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((RNNconfig['batch_size'], RNNconfig['state_size']))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 100 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "training_losses = train_network(num_epochs = 5)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sources (and additional links for reading): \n",
    "\n",
    "1. https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "2. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "3. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
